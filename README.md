# IPL Data Analysis Using Spark

## Overview

This project is used to analyze the Indian Premiere League (IPL) data using Apache Spark. The main aim of the project is to get valuable insights and observe the trends from the dataset in Databricks.

## Architecture

![ipl-pipleine-architecture](https://github.com/user-attachments/assets/9415b584-b968-464c-a5a7-a1936fb98456)

## Tools And Technologies Used

- `Databricks` : platform for efficient big data processing and collaborative analytics
- `AWS S3` : For storing the data in buckets
- `Pandas` : Python library used for smaller data processing tasks
- `Matplotlib` : Python library used for data visualization
- `Apache Spark` : A fast, distributed computing engine for processing large scale datasets

## Objectives

- `Data Ingestion and Cleaning` : Extracted raw data and refined it by handling missing values, duplicates, and inconsistencies to ensure data quality.
- `Data Processing` : Transformed and aggregated data to prepare it for deeper analysis and insights.
- `Visualization` : Created visual representations to highlight key insights and trends in the dataset.
- `Exploring PySpark Fundamentals` : Built foundational skills in PySpark with SQL for handling big data, enabling efficient data manipulation and processing at scale.


## Insights 

  - `Player Performance Analysis` : Analysed the top performers per match and innings.
  - `Toss Imapct` : Assess how winning the toss influences match outcomes, including preferred decisions (bat or field) and success rates.
  - `Player Contribution` : Evaluate player contributions to the match.
  - `Economical Bowlers` : Identified the most economical bowlers in different match phases, such as the powerplay, and visualized this data using Matplotlib for better insights.

## Conclusion

This project serves as a practical introduction to big data analytics, demonstrating Sparkâ€™s capabilities for scalable, real-world data processing and understanding Spark using Python in Databricks.

